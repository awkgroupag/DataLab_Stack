{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Controlboard for different Data Analytics-Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Vanilla-Jupyter-Datascience-Notebook](#Vanilla-Jupyter-Datascience-Notebook)\n",
    "2. [Elastic Stack (formerly ELK-Stack)](#Elastic-Stack-(formerly-ELK-Stack))\n",
    "3. [Neo4j](#Neo4j)\n",
    "4. [Superset](#Superset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Vanilla Jupyter Datascience-Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Your \"standard\" [Jupyter Notebook](https://github.com/jupyter/docker-stacks/tree/master/datascience-notebook), all packages updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pin some variables first\n",
    "\n",
    "Specify the directory you want to mount in order to persist data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir = r\"C:\\Users\\kat\\Documents\\Test-Project\";\n",
    "\n",
    "# Need to convert Windows path for Docker (linux-based)\n",
    "homedir = homedir.replace(\"\\\\\", \"/\").replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker image version/\"tag\" of the Jupyter Datascience-Notebook you want to launch (see the [AWK Repo on Dockerhub](https://hub.docker.com/repository/docker/awkgroupag/datascience-notebook/tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_version = \"e56df3a\"\n",
    "\n",
    "# Hopefully you won't need to use another notebook ;-)\n",
    "notebook = \"awkgroupag/datascience-notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the container (be sure to run the cells above first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker container run -d -p 8888:8888 \\\n",
    "    -e JUPYTER_ENABLE_LAB=yes \\\n",
    "    -v //$homedir:/home/jovyan \\\n",
    "    --name jupyter \\\n",
    "    $notebook:$notebook_version \n",
    "! echo && echo Waiting for 5 seconds for the container to spin up\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "import time\n",
    "\n",
    "time.sleep(5)\n",
    "log = ! sudo docker logs jupyter\n",
    "url = 'http://127.0.0.1:8888'\n",
    "for line in log:\n",
    "    if url in line:\n",
    "        break\n",
    "else:\n",
    "    print(log)\n",
    "    raise RuntimeError('Did not find URL in the log above')\n",
    "url = url + line.split(url, 1)[1]\n",
    "md(f\"**Your Jupyterlab URL is** {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up\n",
    "Stop the Vanilla Jupyter Notebook. Container won't be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker stop jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker rm jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Elastic Stack (formerly ELK-Stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Elasticsearch, Kibana, Beats, and Logstash. Take data from any source, in any format, then search, analyze, and visualize it in real time.\n",
    "\n",
    "* **Elasticsearch** is a distributed, RESTful search and analytics engine. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fineâ€‘tuned relevancy, and powerful analytics that scale with ease.\n",
    "* **Kibana** lets you visualize your Elasticsearch data and navigate the Elastic Stack so you can do anything from tracking query load to understanding the way requests flow through your apps.\n",
    "* **Logstash** is a server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite \"stash.\"\n",
    "* **Beats** is the platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash.\n",
    "\n",
    "Note that Beats (e.g. Metricbeat or Systembeat) are not included in this stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections once the stack has been started\n",
    "* Kibana browser access: [http://localhost:5601](http://localhost:5601)\n",
    "* Elasticsearch access, e.g. through a Jupyter notebook: [http://localhost:9200](http://localhost:9200)\n",
    "* Logstash access: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume create --name=elasticsearch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stack\n",
    "Once pull has completed and containers are running, startup might take 1-2 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker-compose -f \"./elk/docker-compose.yml\" -p \"elk\" up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop and remove the stack (Elasticsearch and Kibana data will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker-compose -f \"./elk/docker-compose.yml\" -p \"elk\" down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all Elasticsearch and Kibana data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume rm elasticsearch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Superset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (under construction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
