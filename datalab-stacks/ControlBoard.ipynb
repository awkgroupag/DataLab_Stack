{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Controlboard for different Data Analytics-Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Vanilla-Jupyter-Datascience-Notebook](#Vanilla-Jupyter-Datascience-Notebook)\n",
    "2. [Elastic Stack (formerly ELK-Stack)](#Elastic-Stack-(formerly-ELK-Stack))\n",
    "3. [PostgreSQL Database](#PostgreSQL-Database)\n",
    "3. [Neo4j](#Neo4j)\n",
    "4. [Manipulate your Docker environment](#Manipulate-your-Docker-environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Vanilla Jupyter Datascience-Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Your \"standard\" [Jupyter Notebook for Datascience](https://github.com/jupyter/docker-stacks/tree/master/datascience-notebook), all packages updated. Missing an important library? Then let the AWK team know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some variables first\n",
    "\n",
    "Specify the Windows directory you want to mount. All **data in the JupyterLab's \"work\" directory will thus survive the destruction of the container**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = r\"C:\\Users\\kat\\Documents\\GitHub\\example-repo\";\n",
    "\n",
    "# Need to convert Windows path for Docker (linux-based)\n",
    "work_dir = work_dir.replace(\"\\\\\", \"/\").replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker image version/\"tag\" of the Jupyter Datascience-Notebook you want to launch (see the [AWK Repo on Dockerhub](https://hub.docker.com/repository/docker/awkgroupag/datascience-notebook/tags) for the latest tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_version = \"0369695\"\n",
    "\n",
    "# You might want to change the Docker image used or the port Jupyter runs under (e.g. if you want to run several Jupyters)\n",
    "notebook = \"awkgroupag/datascience-notebook\"\n",
    "port = 8888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the container (be sure to run the cells above first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker container run -d -p $port:8888 \\\n",
    "    -e JUPYTER_ENABLE_LAB=yes \\\n",
    "    -v \"//$work_dir:/home/jovyan/work\" \\\n",
    "    --name jupyter \\\n",
    "    --network=datalab-network \\\n",
    "    $notebook:$notebook_version \n",
    "! echo && echo Waiting for 5 seconds for the container to spin up && echo\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "import time\n",
    "time.sleep(5)\n",
    "log = ! sudo docker logs jupyter\n",
    "url = 'http://127.0.0.1:8888'\n",
    "for line in log:\n",
    "    if url in line:\n",
    "        break\n",
    "else:\n",
    "    print(log)\n",
    "    raise RuntimeError('Did not find URL in the log above')\n",
    "url = url[:-4] + str(port) + line.split(url, 1)[1]\n",
    "md(f\"**Your Jupyterlab URL is** {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to save your entire computational context if you installed additional packages\n",
    "You might change your Docker container by installing new **PIP** Python packages e.g. with `pip install <package name>`. This change will be lost with the container. To quickly save your entire pip environment, including all packages, copy-paste the following into your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > /home/jovyan/work/pip-environment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your environment again from scratch, e.g. if you re-created your Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r /home/jovyan/work/pip-environment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed additional Python packages with **Anaconda**, `conda install <package name>`, here's how to save the entire conda environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env export -n base > /home/jovyan/work/anaconda-environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-install all Anaconda packages from this file, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env update --name base --file /home/jovyan/work/anaconda-environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up\n",
    "Stop the Vanilla Jupyter Notebook. Container won't be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker stop jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker rm jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Elastic Stack (formerly ELK-Stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Elasticsearch, Kibana, Beats, and Logstash. Take data from any source, in any format, then search, analyze, and visualize it in real time.\n",
    "\n",
    "* **Elasticsearch** is a distributed, RESTful search and analytics engine. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fineâ€‘tuned relevancy, and powerful analytics that scale with ease.\n",
    "* **Kibana** lets you visualize your Elasticsearch data and navigate the Elastic Stack so you can do anything from tracking query load to understanding the way requests flow through your apps.\n",
    "* **Logstash** is a server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite \"stash.\"\n",
    "* **Beats** is the platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash.\n",
    "\n",
    "Note that Beats (e.g. Metricbeat or Systembeat) are not included in this stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections once the stack has been started\n",
    "* Direct Kibana browser access: [http://localhost:5601](http://localhost:5601)\n",
    "* Elasticsearch access for Windows: [http://localhost:9200](http://localhost:9200)\n",
    "* Access Elasticsearch from **within** a Jupyter container: [http://elasticsearch:9200](http://elasticsearch:9200)\n",
    "* Logstash access from **within** a Jupyter container: [http://logstash:9600](http://elasticsearch:9600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume create --name=elasticsearch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stack\n",
    "Once pull has completed and containers are running, startup might take 1-2 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker-compose -f \"./elk/docker-compose.yml\" -p \"elk\" up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop and remove the stack (Elasticsearch and Kibana data will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker-compose -f \"./elk/docker-compose.yml\" -p \"elk\" down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all Elasticsearch and Kibana data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume rm elasticsearch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PostgreSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "[PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) is a powerful, open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume create postgres_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker-compose -f \"./postgres/docker-compose.yml\" -p \"postgres\" up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker-compose -f \"./postgres/docker-compose.yml\" -p \"postgres\" down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all Postgre Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume rm postgres_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get started using another Jupyter Notebook (!!)\n",
    "Start e.g. a Vanilla Jupyterlab instance above, then **copy** the code below\n",
    "#### Connect to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll receive an `Engine`-object from SQLAlchemy (the connection won't be established until you do something with it). [Check here](https://docs.sqlalchemy.org/en/13/core/connections.html) to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from urllib import parse\n",
    "\n",
    "# Name of your database - this database does NOT exist yet (create it below)\n",
    "database = 'my-new-database'\n",
    "\n",
    "# Connection details according to docker-compose.yml - do NOT change this\n",
    "dialect = 'postgresql'  # Could also be MySQL or almost any other DB technology\n",
    "host = 'postgres'\n",
    "port = '5432'\n",
    "username = 'postgres'\n",
    "password = 'password'\n",
    "# URL-encode password for characters like %, Ã¤, ...\n",
    "password = parse.quote_plus(password)\n",
    "\n",
    "url = f'{dialect}://{username}:{password}@{host}:{port}/{database}'\n",
    "engine = create_engine(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new database once\n",
    "Install a dependency first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sqlalchemy_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a NEW database called `my-new-database`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy_utils import database_exists, create_database\n",
    "\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "\n",
    "print(f'Database \"{database}\" exists: {database_exists(engine.url)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load example data into the database\n",
    "Download `northwind_ddl.sql` from [this link](https://raw.githubusercontent.com/YugaByte/yugabyte-db/master/sample/northwind_ddl.sql) and `northwind_data.sql` [from here](https://raw.githubusercontent.com/YugaByte/yugabyte-db/master/sample/northwind_data.sql), both examples taken from the small [Northwind example database](https://dzone.com/articles/how-to-the-northwind-postgresql-sample-database-ru). Move both into your `work` directory mounted in Jupyter.\n",
    "\n",
    "Now just setup the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL statements from the file\n",
    "sql = open(\"/home/jovyan/work/northwind_ddl.sql\").read()\n",
    "# Open a connection and execute the SQL statements\n",
    "with engine.begin() as connection:\n",
    "    connection.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = open(\"/home/jovyan/work/northwind_data.sql\").read()\n",
    "with engine.begin() as connection:\n",
    "    connection.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all tables in the current database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = MetaData()\n",
    "meta.reflect(engine)\n",
    "meta.tables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the example data into a Pandas Dataframe\n",
    "To get you started, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_name = 'customers'\n",
    "df = pd.read_sql_table(table_name, con=engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Manipulate your Docker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all running and stopped Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all Docker images including their filesizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all volumes (=data volumes if you choose to not mount a Windows directory, for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In desperate need to figure out what's eating up your disk space? This command shows where Docker is using disk space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker system df -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate a container\n",
    "Set a container name (or CONTAINER ID) first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker stop $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the running container's logs saved to the Python variable `logoutput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logoutput = ! sudo docker logs $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart an existing (currently stopped) container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker start $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker rm $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up and freeing disk space\n",
    "Remove an image (give either it's name or IMAGE ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"test\"\n",
    "! sudo docker image rm $image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopped containers at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker container prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove a volume (=data volume, thus potentially deleting your data!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = \"test\"\n",
    "! sudo docker volume rm $volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Danger zone**: remove all stopped containers, and all images and all volumes that are currently not associated/mounted with a **running container**. Type the following manually:\n",
    "* Delete all stopped containers, all \"dangling\" images, the build cache, any unattached network: ```! sudo docker system prune --force```\n",
    "* To also delete all currently unused images: ```! sudo docker system prune --all --force```\n",
    "* To also delete all currently unused volumes (potentially deleting your data!): ```! sudo docker system prune --volumes --force```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
